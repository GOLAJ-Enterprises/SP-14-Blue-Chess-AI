{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75b78e31",
   "metadata": {},
   "source": [
    "**Imports & Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af2ce624",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.amp import autocast, GradScaler\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from random import shuffle\n",
    "\n",
    "DATA_DIR = Path(\"prepared_data/batches/\")\n",
    "SAVE_DIR = Path(\"model/\")\n",
    "TRAIN_VAL_RATIO = 0.8\n",
    "VALUE_LOSS_WEIGHT = 0.01\n",
    "LR = 5e-5\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Ensure save directory exists\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80f6080",
   "metadata": {},
   "source": [
    "**Custom PyTorch Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c13f0c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset for loading preprocessed chess training samples.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir: Path, max_samples: int = 100_000):\n",
    "        \"\"\"\n",
    "        Initializes the dataset by loading .pt batch files containing training samples.\n",
    "\n",
    "        :param data_dir: Directory containing .pt files with serialized training data\n",
    "        :param max_samples: Maximum number of samples to load into memory\n",
    "        :raises RuntimeError: If loading any file fails\n",
    "        \"\"\"\n",
    "        self.samples = []\n",
    "        self.batch_paths = list(data_dir.glob(\"*.pt\"))\n",
    "        shuffle(self.batch_paths) # Randomize loading order\n",
    "\n",
    "        loaded_samples = 0\n",
    "        batches_loaded = 0\n",
    "\n",
    "        for path in self.batch_paths:\n",
    "            try:\n",
    "                batch = torch.load(path, weights_only=False)\n",
    "                batches_loaded += 1\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to load {path.name}: {e}\")\n",
    "\n",
    "            # Accumulate individual samples from the loaded batch\n",
    "            for sample in batch:\n",
    "                self.samples.append(sample)\n",
    "                loaded_samples += 1\n",
    "                if loaded_samples >= max_samples:\n",
    "                    break\n",
    "\n",
    "            if loaded_samples >= max_samples:\n",
    "                break\n",
    "\n",
    "        print(f\"Loaded {len(self.samples)} samples from {batches_loaded} batch files.\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the total number of loaded samples.\"\"\"\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        Retrieves the sample at a specific index.\n",
    "\n",
    "        :param idx: Index of the sample\n",
    "        :return: Tuple of (board_tensor, move_index, result_value) as tensors\n",
    "        \"\"\"\n",
    "        board_tensor, move_index, result_value = self.samples[idx]\n",
    "        assert board_tensor.shape == (21, 8, 8), f\"Bad tensor shape: {board_tensor.shape}\"\n",
    "        return (\n",
    "            torch.as_tensor(board_tensor, dtype=torch.float32),\n",
    "            torch.as_tensor(move_index, dtype=torch.long),\n",
    "            torch.as_tensor(result_value, dtype=torch.float32)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cbba6c",
   "metadata": {},
   "source": [
    "**Model Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4781cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoordConv(nn.Module):\n",
    "    \"\"\"Applies convolution after appending coordinate channels to the input tensor.\n",
    "\n",
    "    Adds two additional channels representing normalized X and Y coordinates,\n",
    "    allowing the network to better reason about spatial position.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels + 2, # +2 for the coordinate channels\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            padding=padding,\n",
    "            bias=False # Bias is unnecessary with normalization\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, _, height, width = x.size()\n",
    "\n",
    "        # Generate coordinate grids normalized to [-1, 1]\n",
    "        xx_channel = torch.linspace(-1, 1, width, device=x.device).repeat(height, 1)\n",
    "        yy_channel = torch.linspace(-1, 1, height, device=x.device).repeat(width, 1).t()\n",
    "\n",
    "        # Expand to match batch size and concat as extra channels\n",
    "        xx_channel = xx_channel.unsqueeze(0).expand(batch, -1, -1, -1)\n",
    "        yy_channel = yy_channel.unsqueeze(0).expand(batch, -1, -1, -1)\n",
    "        coords = torch.cat([xx_channel, yy_channel], dim=1)\n",
    "\n",
    "        # Concatenate coordinates to input\n",
    "        x = torch.cat([x, coords], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    \"\"\"Squeeze-and-Excitation block for channel-wise attention.\"\"\"\n",
    "\n",
    "    def __init__(self, channels, reduction=8):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        s = self.pool(x).view(b, c) # Global average pooling\n",
    "        s = self.fc(s).view(b, c, 1, 1) # Channel recalibration\n",
    "        return x * s\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Basic residual block with optional SE block.\"\"\"\n",
    "\n",
    "    def __init__(self, channels, use_se=False):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.norm1 = nn.GroupNorm(8, channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.norm2 = nn.GroupNorm(8, channels)\n",
    "\n",
    "        # Zero-initialize second norm for stable residual learning\n",
    "        nn.init.zeros_(self.norm2.weight)\n",
    "        nn.init.zeros_(self.norm2.bias)\n",
    "\n",
    "        self.se = SEBlock(channels) if use_se else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.norm1(self.conv1(x)))\n",
    "        x = self.norm2(self.conv2(x))\n",
    "        x = self.se(x)\n",
    "        return F.relu(x + residual)\n",
    "\n",
    "\n",
    "class ChessNet(nn.Module):\n",
    "    \"\"\"Main chess policy-value network with CoordConv, residual trunk, and dual heads.\"\"\"\n",
    "\n",
    "    def __init__(self, input_channels=21, num_blocks=8, num_filters=96, policy_out_dim=1968):\n",
    "        super().__init__()\n",
    "\n",
    "        # Input: 21x8x8 tensor (board features)\n",
    "        self.coord_conv = CoordConv(input_channels, num_filters)\n",
    "\n",
    "        # Residual trunk with SE blocks every 3rd block\n",
    "        blocks = []\n",
    "        for i in range(num_blocks):\n",
    "            use_se = (i % 3 == 2)\n",
    "            blocks.append(ResidualBlock(num_filters, use_se=use_se))\n",
    "        self.shared_blocks = nn.Sequential(*blocks)\n",
    "\n",
    "        # Policy head predicts logits over all possible moves\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 2, kernel_size=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2 * 8 * 8, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, policy_out_dim)\n",
    "        )\n",
    "\n",
    "        # Value head predicts the game outcome [-1, 1]\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 1, kernel_size=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(8 * 8, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()  # Output in [-1, 1] for win/loss/draw\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.coord_conv(x))\n",
    "        x = self.shared_blocks(x)\n",
    "\n",
    "        policy_logits = self.policy_head(x)\n",
    "        value = self.value_head(x)\n",
    "\n",
    "        return policy_logits, value.squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d4bacd",
   "metadata": {},
   "source": [
    "**Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bef9f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, policy_loss_fn, value_loss_fn, device, top_k=5, use_amp=False):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the validation set.\n",
    "\n",
    "    :param model: Trained policy-value model\n",
    "    :param val_loader: DataLoader providing validation samples\n",
    "    :param policy_loss_fn: Loss function for the policy head\n",
    "    :param value_loss_fn: Loss function for the value head\n",
    "    :param device: torch.device used for evaluation\n",
    "    :param top_k: Top-K for accuracy computation, defaults to 5\n",
    "    :param use_amp: Whether to use automatic mixed precision (AMP), defaults to False\n",
    "    :return: Tuple of (avg_policy_loss, avg_value_loss, top1_accuracy, topk_accuracy)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_policy_loss = 0\n",
    "    total_value_loss = 0\n",
    "    total_correct_top1 = 0\n",
    "    total_correct_topk = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    val_bar = tqdm(val_loader, desc=\"[Validation]\", unit=\"batch\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for board_tensor, move_index, result_value in val_bar:\n",
    "            board_tensor = board_tensor.to(device)\n",
    "            move_index = move_index.to(device)\n",
    "            result_value = result_value.to(device)\n",
    "\n",
    "            # Forward pass with optional AMP\n",
    "            with autocast(device_type=device.type, enabled=use_amp):\n",
    "                policy_logits, value_preds = model(board_tensor)\n",
    "                policy_loss = policy_loss_fn(policy_logits, move_index)\n",
    "                value_loss = value_loss_fn(value_preds, result_value)\n",
    "\n",
    "            # Top-1 accuracy\n",
    "            predicted_top1 = torch.argmax(policy_logits, dim=1)\n",
    "            correct_top1 = (predicted_top1 == move_index).sum().item()\n",
    "\n",
    "            # Top-k accuracy\n",
    "            topk_preds = torch.topk(policy_logits, top_k, dim=1).indices\n",
    "            correct_topk = (topk_preds == move_index.unsqueeze(1)).any(dim=1).sum().item()\n",
    "\n",
    "            # Update running totals\n",
    "            batch_size = board_tensor.size(0)\n",
    "            total_policy_loss += policy_loss.item() * batch_size\n",
    "            total_value_loss += value_loss.item() * batch_size\n",
    "            total_correct_top1 += correct_top1\n",
    "            total_correct_topk += correct_topk\n",
    "            total_samples += batch_size\n",
    "\n",
    "            # Update averaged metrics for progress bar\n",
    "            avg_policy = total_policy_loss / total_samples\n",
    "            avg_value = total_value_loss / total_samples\n",
    "            avg_total_loss = avg_policy + VALUE_LOSS_WEIGHT * avg_value\n",
    "            acc_top1 = total_correct_top1 / total_samples\n",
    "            acc_topk = total_correct_topk / total_samples\n",
    "\n",
    "            val_bar.set_postfix({\n",
    "                \"policy_loss\": f\"{avg_policy:.4f}\",\n",
    "                \"value_loss\": f\"{avg_value:.4f}\",\n",
    "                \"total_loss\": f\"{avg_total_loss:.4f}\",\n",
    "                \"top1_acc\": f\"{acc_top1:.4f}\",\n",
    "                f\"top{top_k}_acc\": f\"{acc_topk:.4f}\"\n",
    "            })\n",
    "\n",
    "    # Final validation stats\n",
    "    tqdm.write(\n",
    "        f\"[Validation] Policy Loss: {avg_policy:.4f}, \"\n",
    "        f\"Value Loss: {avg_value:.4f}, \"\n",
    "        f\"Total Loss: {avg_total_loss:.4f}, \"\n",
    "        f\"Top-1 Acc: {acc_top1:.4f}, Top-{top_k} Acc: {acc_topk:.4f}\"\n",
    "    )\n",
    "\n",
    "\n",
    "    return avg_policy, avg_value, avg_total_loss, acc_top1, acc_topk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e09bf3",
   "metadata": {},
   "source": [
    "**Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8734b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, scheduler, policy_loss_fn, value_loss_fn, device, epochs=5, validate_after=False, save_best=False, use_amp=False):\n",
    "    \"\"\"\n",
    "    Trains the policy-value network using supervised learning.\n",
    "\n",
    "    :param model: The model to train\n",
    "    :param train_loader: DataLoader for the training set\n",
    "    :param val_loader: DataLoader for the validation set\n",
    "    :param optimizer: Optimizer for training\n",
    "    :param scheduler: Learning rate scheduler (ReduceLROnPlateau)\n",
    "    :param policy_loss_fn: Loss function for policy head\n",
    "    :param value_loss_fn: Loss function for value head\n",
    "    :param device: torch.device to run training on\n",
    "    :param epochs: Number of training epochs, defaults to 5\n",
    "    :param validate_after: Whether to run validation after each epoch, defaults to False\n",
    "    :param save_best: Whether to save the best model during training, defaults to False\n",
    "    :param use_amp: Use automatic mixed precision (AMP), defaults to False\n",
    "    \"\"\"\n",
    "\n",
    "    best_metric = float(\"-inf\")  # Used to track the best model (by accuracy or loss)\n",
    "    LOG_PATH = SAVE_DIR / \"train_log.log\"\n",
    "    scaler = GradScaler(device=device.type, enabled=use_amp)\n",
    "\n",
    "    # Initialize training log\n",
    "    with open(LOG_PATH, \"w\") as log_file:\n",
    "        log_file.write(\"epoch | train_policy_loss - train_value_loss - train_total_loss | val_policy_loss - val_value_loss - val_total_loss | top1_acc - top5_acc | saved\\n\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_policy_loss = 0\n",
    "        total_value_loss = 0\n",
    "        total_samples = 0\n",
    "        saved_model = False\n",
    "\n",
    "        epoch_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\", leave=False)\n",
    "\n",
    "        for board_tensor, move_index, result_value in epoch_bar:\n",
    "            board_tensor = board_tensor.to(device)\n",
    "            move_index = move_index.to(device)\n",
    "            result_value = result_value.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with AMP\n",
    "            with autocast(device_type=device.type, enabled=use_amp):\n",
    "                policy_logits, value_preds = model(board_tensor)\n",
    "                policy_loss = policy_loss_fn(policy_logits, move_index)\n",
    "                value_loss = value_loss_fn(value_preds, result_value)\n",
    "                loss = policy_loss + VALUE_LOSS_WEIGHT * value_loss\n",
    "\n",
    "            # Backward and optimizer step\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # Update running loss stats\n",
    "            batch_size = board_tensor.size(0)\n",
    "            total_policy_loss += policy_loss.detach().item() * batch_size\n",
    "            total_value_loss += value_loss.detach().item() * batch_size\n",
    "            total_samples += batch_size\n",
    "\n",
    "            avg_policy = total_policy_loss / total_samples\n",
    "            avg_value = total_value_loss / total_samples\n",
    "            avg_total_loss = avg_policy + VALUE_LOSS_WEIGHT * avg_value\n",
    "\n",
    "            # Update progress bar\n",
    "            epoch_bar.set_postfix({\n",
    "                \"policy_loss\": f\"{avg_policy:.4f}\",\n",
    "                \"value_loss\": f\"{avg_value:.4f}\",\n",
    "                \"total_loss\": f\"{avg_total_loss:.4f}\"\n",
    "            })\n",
    "\n",
    "        tqdm.write(f\"[Epoch {epoch+1}] Train Policy Loss: {avg_policy:.4f}, Value Loss: {avg_value:.4f}\")\n",
    "\n",
    "        val_policy_loss = val_value_loss = val_total_loss = acc_top1 = acc_topk = float(\"nan\")\n",
    "\n",
    "        if validate_after:\n",
    "            # Evaluate on validation set\n",
    "            val_policy_loss, val_value_loss, val_total_loss, acc_top1, acc_topk = validate(\n",
    "                model, val_loader, policy_loss_fn, value_loss_fn, device, top_k=5, use_amp=use_amp\n",
    "            )\n",
    "            scheduler.step(val_total_loss)\n",
    "\n",
    "            # Save best model by validation accuracy\n",
    "            if save_best and acc_top1 > best_metric:\n",
    "                best_metric = acc_top1\n",
    "                model.eval()\n",
    "                save_path = SAVE_DIR / f\"model_epoch{epoch+1:02d}_acc{acc_top1:.4f}.pt\"\n",
    "                scripted = torch.jit.script(model.cpu())\n",
    "                scripted.save(str(save_path))\n",
    "                model.to(device)\n",
    "                saved_model = True\n",
    "                tqdm.write(f\"Saved new best model (Top-1 Accuracy: {best_metric:.4f})\")\n",
    "\n",
    "        elif save_best:\n",
    "            # Save best model by lowest training loss\n",
    "            current_metric = -(avg_policy + VALUE_LOSS_WEIGHT * avg_value)\n",
    "            if current_metric > best_metric:\n",
    "                best_metric = current_metric\n",
    "                model.eval()\n",
    "                save_path = SAVE_DIR / f\"model_epoch{epoch+1:02d}_trainloss{-current_metric:.4f}.pt\"\n",
    "                scripted = torch.jit.script(model.cpu())\n",
    "                scripted.save(str(save_path))\n",
    "                model.to(device)\n",
    "                saved_model = True\n",
    "                tqdm.write(f\"Saved new best scripted model (Train Loss: {-best_metric:.4f})\")\n",
    "\n",
    "        # Append results to log\n",
    "        with open(LOG_PATH, \"a\") as log_file:\n",
    "            log_file.write(\n",
    "                f\"{epoch+1:02d} | {avg_policy:.4f} - {avg_value:.4f} - {avg_total_loss:.4f} | {val_policy_loss:.4f} - \"\n",
    "                f\"{val_value_loss:.4f} - {val_total_loss:.4f} | {acc_top1:.4f} - {acc_topk:.4f} | {int(saved_model)}\\n\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56c7982",
   "metadata": {},
   "source": [
    "**Run Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a362615e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4000000 samples from 400 batch files.\n"
     ]
    }
   ],
   "source": [
    "dataset = ChessDataset(DATA_DIR, max_samples=4_000_000)\n",
    "\n",
    "# Compute sizes\n",
    "total_samples = len(dataset)\n",
    "train_size = int(total_samples * TRAIN_VAL_RATIO)\n",
    "val_size = total_samples - train_size\n",
    "\n",
    "# Split randomly\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_set, val_set = random_split(dataset, [train_size, val_size], generator=generator)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcca3cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 1,898,185\n",
      "Number of policy output classes: 1968\n",
      "Value head output size: 1\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ChessNet(num_blocks=8, num_filters=96).to(device)\n",
    "\n",
    "# Loss functions\n",
    "policy_loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "value_loss_fn = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "\n",
    "# LR scheduler\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5, # halve the LR\n",
    "    patience=2 # wait 2 epochs with no improvement\n",
    ")\n",
    "\n",
    "# Extract policy and value head output sizes\n",
    "policy_out_features = model.policy_head[-1].out_features # final Linear in policy head\n",
    "value_out_features = model.value_head[-2].out_features # second to last Linear (before tanh)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"Number of policy output classes: {policy_out_features}\")\n",
    "print(f\"Value head output size: {value_out_features}\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80b1582d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Policy Loss: 4.6979, Value Loss: 0.8688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Policy Loss: 3.9068, Value Loss: 0.8657, Total Loss: 3.9155, Top-1 Acc: 0.2471, Top-5 Acc: 0.5460\n",
      "Saved new best model (Top-1 Accuracy: 0.2471)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Policy Loss: 3.6953, Value Loss: 0.8615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Policy Loss: 3.5646, Value Loss: 0.8583, Total Loss: 3.5732, Top-1 Acc: 0.3009, Top-5 Acc: 0.6292\n",
      "Saved new best model (Top-1 Accuracy: 0.3009)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Policy Loss: 3.4415, Value Loss: 0.8557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Policy Loss: 3.3918, Value Loss: 0.8533, Total Loss: 3.4003, Top-1 Acc: 0.3327, Top-5 Acc: 0.6761\n",
      "Saved new best model (Top-1 Accuracy: 0.3327)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Policy Loss: 3.2917, Value Loss: 0.8508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Policy Loss: 3.2810, Value Loss: 0.8471, Total Loss: 3.2895, Top-1 Acc: 0.3538, Top-5 Acc: 0.7051\n",
      "Saved new best model (Top-1 Accuracy: 0.3538)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Policy Loss: 3.1856, Value Loss: 0.8461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Policy Loss: 3.1959, Value Loss: 0.8448, Total Loss: 3.2043, Top-1 Acc: 0.3706, Top-5 Acc: 0.7294\n",
      "Saved new best model (Top-1 Accuracy: 0.3706)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train Policy Loss: 3.1049, Value Loss: 0.8426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Policy Loss: 3.1418, Value Loss: 0.8429, Total Loss: 3.1502, Top-1 Acc: 0.3817, Top-5 Acc: 0.7459\n",
      "Saved new best model (Top-1 Accuracy: 0.3817)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train Policy Loss: 3.0413, Value Loss: 0.8398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Policy Loss: 3.0901, Value Loss: 0.8387, Total Loss: 3.0985, Top-1 Acc: 0.3900, Top-5 Acc: 0.7583\n",
      "Saved new best model (Top-1 Accuracy: 0.3900)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train Policy Loss: 2.9899, Value Loss: 0.8376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Policy Loss: 3.0613, Value Loss: 0.8347, Total Loss: 3.0696, Top-1 Acc: 0.3947, Top-5 Acc: 0.7674\n",
      "Saved new best model (Top-1 Accuracy: 0.3947)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train Policy Loss: 2.9474, Value Loss: 0.8360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Policy Loss: 3.0292, Value Loss: 0.8366, Total Loss: 3.0376, Top-1 Acc: 0.4014, Top-5 Acc: 0.7760\n",
      "Saved new best model (Top-1 Accuracy: 0.4014)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Train Policy Loss: 2.9112, Value Loss: 0.8348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Policy Loss: 3.0070, Value Loss: 0.8387, Total Loss: 3.0154, Top-1 Acc: 0.4076, Top-5 Acc: 0.7806\n",
      "Saved new best model (Top-1 Accuracy: 0.4076)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Train Policy Loss: 2.8804, Value Loss: 0.8337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Policy Loss: 2.9861, Value Loss: 0.8381, Total Loss: 2.9945, Top-1 Acc: 0.4116, Top-5 Acc: 0.7872\n",
      "Saved new best model (Top-1 Accuracy: 0.4116)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Train Policy Loss: 2.8537, Value Loss: 0.8327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Policy Loss: 2.9763, Value Loss: 0.8338, Total Loss: 2.9846, Top-1 Acc: 0.4133, Top-5 Acc: 0.7902\n",
      "Saved new best model (Top-1 Accuracy: 0.4133)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Train Policy Loss: 2.8296, Value Loss: 0.8322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Policy Loss: 2.9624, Value Loss: 0.8318, Total Loss: 2.9707, Top-1 Acc: 0.4169, Top-5 Acc: 0.7933\n",
      "Saved new best model (Top-1 Accuracy: 0.4169)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Train Policy Loss: 2.8083, Value Loss: 0.8311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Policy Loss: 2.9506, Value Loss: 0.8317, Total Loss: 2.9589, Top-1 Acc: 0.4172, Top-5 Acc: 0.7953\n",
      "Saved new best model (Top-1 Accuracy: 0.4172)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Train Policy Loss: 2.7886, Value Loss: 0.8305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Policy Loss: 2.9393, Value Loss: 0.8296, Total Loss: 2.9476, Top-1 Acc: 0.4216, Top-5 Acc: 0.7986\n",
      "Saved new best model (Top-1 Accuracy: 0.4216)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16] Train Policy Loss: 2.7710, Value Loss: 0.8300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Policy Loss: 2.9275, Value Loss: 0.8292, Total Loss: 2.9358, Top-1 Acc: 0.4231, Top-5 Acc: 0.8011\n",
      "Saved new best model (Top-1 Accuracy: 0.4231)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17] Train Policy Loss: 2.7546, Value Loss: 0.8294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Policy Loss: 2.9239, Value Loss: 0.8290, Total Loss: 2.9322, Top-1 Acc: 0.4257, Top-5 Acc: 0.8043\n",
      "Saved new best model (Top-1 Accuracy: 0.4257)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18] Train Policy Loss: 2.7397, Value Loss: 0.8291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Policy Loss: 2.9165, Value Loss: 0.8287, Total Loss: 2.9248, Top-1 Acc: 0.4277, Top-5 Acc: 0.8048\n",
      "Saved new best model (Top-1 Accuracy: 0.4277)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19] Train Policy Loss: 2.7255, Value Loss: 0.8287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Policy Loss: 2.9232, Value Loss: 0.8296, Total Loss: 2.9315, Top-1 Acc: 0.4267, Top-5 Acc: 0.8051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20] Train Policy Loss: 2.7122, Value Loss: 0.8282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Policy Loss: 2.9120, Value Loss: 0.8286, Total Loss: 2.9203, Top-1 Acc: 0.4274, Top-5 Acc: 0.8063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 21] Train Policy Loss: 2.6999, Value Loss: 0.8278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Policy Loss: 2.9119, Value Loss: 0.8280, Total Loss: 2.9202, Top-1 Acc: 0.4276, Top-5 Acc: 0.8067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 22] Train Policy Loss: 2.6883, Value Loss: 0.8275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Policy Loss: 2.9069, Value Loss: 0.8272, Total Loss: 2.9151, Top-1 Acc: 0.4312, Top-5 Acc: 0.8086\n",
      "Saved new best model (Top-1 Accuracy: 0.4312)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23] Train Policy Loss: 2.6770, Value Loss: 0.8274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Policy Loss: 2.9110, Value Loss: 0.8273, Total Loss: 2.9193, Top-1 Acc: 0.4312, Top-5 Acc: 0.8090\n",
      "Saved new best model (Top-1 Accuracy: 0.4312)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 24] Train Policy Loss: 2.6669, Value Loss: 0.8272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Policy Loss: 2.9083, Value Loss: 0.8291, Total Loss: 2.9166, Top-1 Acc: 0.4298, Top-5 Acc: 0.8091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 25] Train Policy Loss: 2.6569, Value Loss: 0.8270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Policy Loss: 2.9052, Value Loss: 0.8281, Total Loss: 2.9135, Top-1 Acc: 0.4327, Top-5 Acc: 0.8090\n",
      "Saved new best model (Top-1 Accuracy: 0.4327)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 26] Train Policy Loss: 2.6473, Value Loss: 0.8271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Policy Loss: 2.9005, Value Loss: 0.8299, Total Loss: 2.9088, Top-1 Acc: 0.4343, Top-5 Acc: 0.8109\n",
      "Saved new best model (Top-1 Accuracy: 0.4343)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 27] Train Policy Loss: 2.6384, Value Loss: 0.8269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Policy Loss: 2.9053, Value Loss: 0.8289, Total Loss: 2.9136, Top-1 Acc: 0.4320, Top-5 Acc: 0.8108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 28] Train Policy Loss: 2.6297, Value Loss: 0.8267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Policy Loss: 2.9038, Value Loss: 0.8283, Total Loss: 2.9121, Top-1 Acc: 0.4335, Top-5 Acc: 0.8115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 29] Train Policy Loss: 2.6216, Value Loss: 0.8266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Policy Loss: 2.9111, Value Loss: 0.8273, Total Loss: 2.9193, Top-1 Acc: 0.4318, Top-5 Acc: 0.8099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 30] Train Policy Loss: 2.5476, Value Loss: 0.8245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Policy Loss: 2.8844, Value Loss: 0.8265, Total Loss: 2.8926, Top-1 Acc: 0.4378, Top-5 Acc: 0.8165\n",
      "Saved new best model (Top-1 Accuracy: 0.4378)\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    policy_loss_fn=policy_loss_fn,\n",
    "    value_loss_fn=value_loss_fn,\n",
    "    device=device,\n",
    "    epochs=EPOCHS,\n",
    "    validate_after=True,\n",
    "    save_best=True,\n",
    "    use_amp=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
